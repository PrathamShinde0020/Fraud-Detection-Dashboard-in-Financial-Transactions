{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88295e34-13aa-4081-be60-ead4207ad522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imblearn\n",
      "  Downloading imblearn-0.0-py2.py3-none-any.whl.metadata (355 bytes)\n",
      "Collecting imbalanced-learn (from imblearn)\n",
      "  Using cached imbalanced_learn-0.13.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.24.3 in c:\\users\\prathamesh\\jupyter_env\\lib\\site-packages (from imbalanced-learn->imblearn) (2.3.1)\n",
      "Requirement already satisfied: scipy<2,>=1.10.1 in c:\\users\\prathamesh\\jupyter_env\\lib\\site-packages (from imbalanced-learn->imblearn) (1.16.0)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.3.2 in c:\\users\\prathamesh\\jupyter_env\\lib\\site-packages (from imbalanced-learn->imblearn) (1.7.0)\n",
      "Collecting sklearn-compat<1,>=0.1 (from imbalanced-learn->imblearn)\n",
      "  Using cached sklearn_compat-0.1.3-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: joblib<2,>=1.1.1 in c:\\users\\prathamesh\\jupyter_env\\lib\\site-packages (from imbalanced-learn->imblearn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in c:\\users\\prathamesh\\jupyter_env\\lib\\site-packages (from imbalanced-learn->imblearn) (3.6.0)\n",
      "Collecting scikit-learn<2,>=1.3.2 (from imbalanced-learn->imblearn)\n",
      "  Using cached scikit_learn-1.6.1-cp313-cp313-win_amd64.whl.metadata (15 kB)\n",
      "Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
      "Using cached imbalanced_learn-0.13.0-py3-none-any.whl (238 kB)\n",
      "Using cached sklearn_compat-0.1.3-py3-none-any.whl (18 kB)\n",
      "Using cached scikit_learn-1.6.1-cp313-cp313-win_amd64.whl (11.1 MB)\n",
      "Installing collected packages: scikit-learn, sklearn-compat, imbalanced-learn, imblearn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.7.0\n",
      "    Uninstalling scikit-learn-1.7.0:\n",
      "      Successfully uninstalled scikit-learn-1.7.0\n",
      "Successfully installed imbalanced-learn-0.13.0 imblearn-0.0 scikit-learn-1.6.1 sklearn-compat-0.1.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Prathamesh\\jupyter_env\\Lib\\site-packages\\~klearn'.\n",
      "  You can safely remove it manually.\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9aef367c-d80e-4f0f-93c0-fbb8adb3ccc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è imbalanced-learn not available. Using alternative class balancing methods.\n",
      "üöÄ Enhanced Fraud Detection System Initialized\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Fraud Detection System - Complete ML Pipeline\n",
    "# Author: Data Science Team\n",
    "# Objective: Build a robust fraud detection system with high precision and recall\n",
    "# Version: 2.0 - Enhanced with better accuracy and fixed encoding issues\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, \n",
    "                           roc_auc_score, roc_curve, precision_recall_curve,\n",
    "                           f1_score, precision_score, recall_score, average_precision_score)\n",
    "\n",
    "# Advanced ML\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    HAS_XGBOOST = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è XGBoost not available. Skipping XGBoost model.\")\n",
    "    HAS_XGBOOST = False\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostClassifier\n",
    "    HAS_CATBOOST = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è CatBoost not available. Skipping CatBoost model.\")\n",
    "    HAS_CATBOOST = False\n",
    "\n",
    "# Imbalanced Learning\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE\n",
    "    from imblearn.under_sampling import RandomUnderSampler, EditedNearestNeighbours\n",
    "    from imblearn.combine import SMOTETomek\n",
    "    from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "    HAS_IMBLEARN = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è imbalanced-learn not available. Using alternative class balancing methods.\")\n",
    "    HAS_IMBLEARN = False\n",
    "\n",
    "# Utilities\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "# Set style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üöÄ Enhanced Fraud Detection System Initialized\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. ENHANCED DATA SIMULATION WITH REALISTIC PATTERNS\n",
    "# ============================================================================\n",
    "\n",
    "def create_enhanced_sample_data(n_samples=10000):\n",
    "    \"\"\"Create enhanced sample data with more realistic fraud patterns\"\"\"\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    \n",
    "    print(f\"üìä Generating {n_samples:,} sample transactions...\")\n",
    "    \n",
    "    # Generate base features\n",
    "    data = {\n",
    "        'Transaction_ID': [f'TXN_{i:06d}' for i in range(n_samples)],\n",
    "        'User_ID': [f'USER_{np.random.randint(1000, 9999)}' for _ in range(n_samples)],\n",
    "    }\n",
    "    \n",
    "    # Generate correlated features for more realistic data\n",
    "    # Risk score influences many other features\n",
    "    risk_scores = np.random.beta(2, 5, n_samples)  # Skewed towards lower values\n",
    "    data['Risk_Score'] = risk_scores\n",
    "    \n",
    "    # Transaction amount - higher amounts more likely with high risk\n",
    "    base_amounts = np.random.lognormal(3.0, 1.5, n_samples)\n",
    "    risk_multiplier = 1 + 2 * risk_scores  # High risk = higher amounts\n",
    "    data['Transaction_Amount'] = np.clip(base_amounts * risk_multiplier, 1, 10000)\n",
    "    \n",
    "    # Account balance - inversely related to risk sometimes\n",
    "    data['Account_Balance'] = np.random.lognormal(8, 1.2, n_samples)\n",
    "    data['Account_Balance'] = np.clip(data['Account_Balance'], 500, 100000)\n",
    "    \n",
    "    # Transaction types with different fraud propensities\n",
    "    transaction_types = ['POS', 'Bank Transfer', 'Online', 'ATM Withdrawal']\n",
    "    type_probs = [0.4, 0.25, 0.25, 0.1]  # Online and transfers riskier\n",
    "    data['Transaction_Type'] = np.random.choice(transaction_types, n_samples, p=type_probs)\n",
    "    \n",
    "    # Device types\n",
    "    device_types = ['Mobile', 'Laptop', 'Tablet']\n",
    "    data['Device_Type'] = np.random.choice(device_types, n_samples, p=[0.6, 0.3, 0.1])\n",
    "    \n",
    "    # Locations with varying risk levels\n",
    "    locations = ['New York', 'London', 'Sydney', 'Mumbai', 'Tokyo', 'Lagos', 'Moscow']\n",
    "    data['Location'] = np.random.choice(locations, n_samples)\n",
    "    \n",
    "    # Merchant categories\n",
    "    merchant_cats = ['Grocery', 'Gas', 'Restaurants', 'Clothing', 'Electronics', 'Travel', 'Entertainment']\n",
    "    data['Merchant_Category'] = np.random.choice(merchant_cats, n_samples)\n",
    "    \n",
    "    # IP flag - correlated with risk score\n",
    "    ip_flag_prob = 0.02 + 0.15 * risk_scores\n",
    "    data['IP_Address_Flag'] = np.random.binomial(1, ip_flag_prob, n_samples)\n",
    "    \n",
    "    # Previous fraudulent activity\n",
    "    prev_fraud_prob = 0.05 + 0.2 * risk_scores\n",
    "    data['Previous_Fraudulent_Activity'] = np.random.binomial(1, prev_fraud_prob, n_samples)\n",
    "    \n",
    "    # Daily transaction count - high activity can be suspicious\n",
    "    daily_count_base = np.random.poisson(3, n_samples)\n",
    "    daily_count_risk = np.random.poisson(risk_scores * 10, n_samples)\n",
    "    data['Daily_Transaction_Count'] = np.clip(daily_count_base + daily_count_risk, 1, 20)\n",
    "    \n",
    "    # 7-day average amount\n",
    "    data['Avg_Transaction_Amount_7d'] = data['Transaction_Amount'] * np.random.uniform(0.7, 1.3, n_samples)\n",
    "    \n",
    "    # Failed transaction count - higher for risky users\n",
    "    failed_base_prob = 0.8 * risk_scores\n",
    "    data['Failed_Transaction_Count_7d'] = np.random.poisson(failed_base_prob * 5, n_samples)\n",
    "    data['Failed_Transaction_Count_7d'] = np.clip(data['Failed_Transaction_Count_7d'], 0, 10)\n",
    "    \n",
    "    # Card types\n",
    "    card_types = ['Visa', 'Mastercard', 'Amex']\n",
    "    data['Card_Type'] = np.random.choice(card_types, n_samples, p=[0.5, 0.35, 0.15])\n",
    "    \n",
    "    # Card age - newer cards might be riskier\n",
    "    data['Card_Age'] = np.random.exponential(100, n_samples).astype(int)\n",
    "    data['Card_Age'] = np.clip(data['Card_Age'], 1, 2000)\n",
    "    \n",
    "    # Transaction distance - higher distances more suspicious\n",
    "    distance_base = np.random.exponential(50, n_samples)\n",
    "    distance_risk = risk_scores * np.random.exponential(500, n_samples)\n",
    "    data['Transaction_Distance'] = distance_base + distance_risk\n",
    "    data['Transaction_Distance'] = np.clip(data['Transaction_Distance'], 0.1, 8000)\n",
    "    \n",
    "    # Authentication methods\n",
    "    auth_methods = ['Password', 'Biometric', 'OTP', '2FA']\n",
    "    data['Authentication_Method'] = np.random.choice(auth_methods, n_samples, p=[0.4, 0.3, 0.2, 0.1])\n",
    "    \n",
    "    # Weekend flag\n",
    "    data['Is_Weekend'] = np.random.binomial(1, 0.28, n_samples)\n",
    "    \n",
    "    # Generate realistic timestamps\n",
    "    start_date = pd.Timestamp('2023-01-01')\n",
    "    end_date = pd.Timestamp('2023-12-31')\n",
    "    timestamps = pd.date_range(start_date, end_date, periods=n_samples)\n",
    "    data['Timestamp'] = np.random.choice(timestamps, n_samples)\n",
    "    \n",
    "    df = pd.read_csv('fraud_dataset.csv')\n",
    "    \n",
    "    # Generate fraud labels with complex realistic patterns\n",
    "    fraud_probability = (\n",
    "        0.05 +  # Base fraud rate\n",
    "        0.35 * (df['Risk_Score'] > 0.7) +  # High risk score\n",
    "        0.25 * (df['Transaction_Amount'] > df['Transaction_Amount'].quantile(0.95)) +  # Very high amounts\n",
    "        0.3 * df['IP_Address_Flag'] +  # Suspicious IP\n",
    "        0.4 * df['Previous_Fraudulent_Activity'] +  # Previous fraud history\n",
    "        0.2 * (df['Failed_Transaction_Count_7d'] > 3) +  # Multiple recent failures\n",
    "        0.15 * (df['Transaction_Distance'] > df['Transaction_Distance'].quantile(0.9)) +  # Far transactions\n",
    "        0.1 * (df['Daily_Transaction_Count'] > 10) +  # High daily activity\n",
    "        0.08 * (df['Transaction_Type'] == 'Online') +  # Online transactions riskier\n",
    "        0.06 * (df['Authentication_Method'] == 'Password') +  # Weak auth\n",
    "        0.05 * df['Is_Weekend'] +  # Weekend transactions\n",
    "        0.1 * (df['Card_Age'] < 30)  # New cards\n",
    "    )\n",
    "    \n",
    "    # Add some noise and cap probability\n",
    "    fraud_probability += np.random.normal(0, 0.05, n_samples)\n",
    "    fraud_probability = np.clip(fraud_probability, 0.01, 0.85)\n",
    "    \n",
    "    # Generate labels\n",
    "    df['Fraud_Label'] = np.random.binomial(1, fraud_probability, n_samples)\n",
    "    \n",
    "    print(f\"‚úÖ Data generated successfully!\")\n",
    "    print(f\"üìä Fraud rate: {df['Fraud_Label'].mean():.1%}\")\n",
    "    print(f\"üìà High-risk transactions: {(df['Risk_Score'] > 0.7).mean():.1%}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ============================================================================\n",
    "# 2. ADVANCED FEATURE ENGINEERING\n",
    "# ============================================================================\n",
    "\n",
    "def advanced_feature_engineering(df):\n",
    "    \"\"\"Enhanced feature engineering with domain knowledge\"\"\"\n",
    "    \n",
    "    print(\"\\nüîß ADVANCED FEATURE ENGINEERING\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # 1. Time-based features (enhanced)\n",
    "    print(\"‚è∞ Creating advanced time features...\")\n",
    "    df_processed['Timestamp'] = pd.to_datetime(df_processed['Timestamp'])\n",
    "    df_processed['Hour'] = df_processed['Timestamp'].dt.hour\n",
    "    df_processed['Day'] = df_processed['Timestamp'].dt.day\n",
    "    df_processed['Month'] = df_processed['Timestamp'].dt.month\n",
    "    df_processed['DayOfWeek'] = df_processed['Timestamp'].dt.dayofweek\n",
    "    df_processed['Quarter'] = df_processed['Timestamp'].dt.quarter\n",
    "    df_processed['IsWeekend'] = df_processed['Timestamp'].dt.weekday >= 5\n",
    "    \n",
    "    # Time-based risk categories\n",
    "    df_processed['Is_Night'] = ((df_processed['Hour'] >= 23) | (df_processed['Hour'] <= 5)).astype(int)\n",
    "    df_processed['Is_Business_Hours'] = ((df_processed['Hour'] >= 9) & (df_processed['Hour'] <= 17)).astype(int)\n",
    "    df_processed['Is_Late_Evening'] = ((df_processed['Hour'] >= 20) & (df_processed['Hour'] < 23)).astype(int)\n",
    "    df_processed['Is_Early_Morning'] = ((df_processed['Hour'] >= 6) & (df_processed['Hour'] < 9)).astype(int)\n",
    "    \n",
    "    # 2. Amount-based features (enhanced)\n",
    "    print(\"üí∞ Creating sophisticated amount features...\")\n",
    "    df_processed['Amount_to_Balance_Ratio'] = df_processed['Transaction_Amount'] / (df_processed['Account_Balance'] + 1)\n",
    "    df_processed['Amount_vs_7d_Avg'] = df_processed['Transaction_Amount'] / (df_processed['Avg_Transaction_Amount_7d'] + 1)\n",
    "    \n",
    "    # Amount percentiles and categories\n",
    "    df_processed['Amount_Percentile'] = df_processed['Transaction_Amount'].rank(pct=True)\n",
    "    df_processed['Is_Large_Transaction'] = (df_processed['Amount_Percentile'] > 0.95).astype(int)\n",
    "    df_processed['Is_Small_Transaction'] = (df_processed['Amount_Percentile'] < 0.1).astype(int)\n",
    "    df_processed['Is_Round_Amount'] = (df_processed['Transaction_Amount'] % 100 == 0).astype(int)\n",
    "    \n",
    "    # Log transformations for skewed features\n",
    "    df_processed['Log_Transaction_Amount'] = np.log1p(df_processed['Transaction_Amount'])\n",
    "    df_processed['Log_Account_Balance'] = np.log1p(df_processed['Account_Balance'])\n",
    "    df_processed['Log_Transaction_Distance'] = np.log1p(df_processed['Transaction_Distance'])\n",
    "    \n",
    "    # 3. Risk-based features (enhanced)\n",
    "    print(\"‚ö†Ô∏è Creating comprehensive risk features...\")\n",
    "    df_processed['Risk_Category'] = pd.cut(df_processed['Risk_Score'], \n",
    "                                         bins=[0, 0.2, 0.5, 0.8, 1.0], \n",
    "                                         labels=['Very_Low', 'Low', 'Medium', 'High'])\n",
    "    df_processed['High_Risk_Flag'] = (df_processed['Risk_Score'] > 0.7).astype(int)\n",
    "    df_processed['Very_High_Risk_Flag'] = (df_processed['Risk_Score'] > 0.9).astype(int)\n",
    "    df_processed['Risk_Score_Squared'] = df_processed['Risk_Score'] ** 2\n",
    "    \n",
    "    # 4. Behavioral features (enhanced)\n",
    "    print(\"üéØ Creating behavioral pattern features...\")\n",
    "    df_processed['High_Daily_Activity'] = (df_processed['Daily_Transaction_Count'] > df_processed['Daily_Transaction_Count'].quantile(0.8)).astype(int)\n",
    "    df_processed['Very_High_Activity'] = (df_processed['Daily_Transaction_Count'] > df_processed['Daily_Transaction_Count'].quantile(0.95)).astype(int)\n",
    "    df_processed['Multiple_Failures'] = (df_processed['Failed_Transaction_Count_7d'] > 1).astype(int)\n",
    "    df_processed['Many_Failures'] = (df_processed['Failed_Transaction_Count_7d'] > 3).astype(int)\n",
    "    df_processed['Far_Transaction'] = (df_processed['Transaction_Distance'] > df_processed['Transaction_Distance'].quantile(0.8)).astype(int)\n",
    "    df_processed['Very_Far_Transaction'] = (df_processed['Transaction_Distance'] > df_processed['Transaction_Distance'].quantile(0.95)).astype(int)\n",
    "    \n",
    "    # Failure rate\n",
    "    df_processed['Failure_Rate'] = df_processed['Failed_Transaction_Count_7d'] / (df_processed['Daily_Transaction_Count'] + 1)\n",
    "    \n",
    "    # 5. Card and authentication features\n",
    "    print(\"üí≥ Creating card and authentication features...\")\n",
    "    df_processed['Is_New_Card'] = (df_processed['Card_Age'] < 30).astype(int)\n",
    "    df_processed['Is_Old_Card'] = (df_processed['Card_Age'] > 365).astype(int)\n",
    "    df_processed['Is_Weak_Auth'] = (df_processed['Authentication_Method'] == 'Password').astype(int)\n",
    "    df_processed['Is_Strong_Auth'] = (df_processed['Authentication_Method'].isin(['Biometric', '2FA'])).astype(int)\n",
    "    \n",
    "    # 6. Interaction features\n",
    "    print(\"üîó Creating interaction features...\")\n",
    "    df_processed['Risk_Amount_Interaction'] = df_processed['Risk_Score'] * df_processed['Log_Transaction_Amount']\n",
    "    df_processed['Risk_Distance_Interaction'] = df_processed['Risk_Score'] * df_processed['Log_Transaction_Distance']\n",
    "    df_processed['Amount_Distance_Interaction'] = df_processed['Log_Transaction_Amount'] * df_processed['Log_Transaction_Distance']\n",
    "    df_processed['Night_Weekend_Interaction'] = df_processed['Is_Night'] * df_processed['IsWeekend']\n",
    "    df_processed['High_Risk_High_Amount'] = df_processed['High_Risk_Flag'] * df_processed['Is_Large_Transaction']\n",
    "    \n",
    "    # 7. Frequency encoding for categorical variables\n",
    "    print(\"üìä Creating frequency encodings...\")\n",
    "    for col in ['Location', 'Merchant_Category', 'Device_Type']:\n",
    "        freq_map = df_processed[col].value_counts(normalize=True).to_dict()\n",
    "        df_processed[f'{col}_Frequency'] = df_processed[col].map(freq_map)\n",
    "    \n",
    "    # 8. Categorical encoding with label encoding\n",
    "    print(\"üè∑Ô∏è Encoding categorical variables...\")\n",
    "    categorical_cols = ['Transaction_Type', 'Device_Type', 'Location', 'Merchant_Category', \n",
    "                       'Card_Type', 'Authentication_Method', 'Risk_Category']\n",
    "    \n",
    "    le_dict = {}\n",
    "    for col in categorical_cols:\n",
    "        if col in df_processed.columns:\n",
    "            le = LabelEncoder()\n",
    "            df_processed[f'{col}_encoded'] = le.fit_transform(df_processed[col].astype(str))\n",
    "            le_dict[col] = le\n",
    "    \n",
    "    # Drop original categorical columns and identifiers\n",
    "    cols_to_drop = ['Transaction_ID', 'User_ID', 'Timestamp'] + \\\n",
    "                   [col for col in categorical_cols if col in df_processed.columns]\n",
    "    df_processed = df_processed.drop(columns=cols_to_drop)\n",
    "    \n",
    "    print(f\"‚úÖ Advanced feature engineering complete!\")\n",
    "    print(f\"üìä Final feature count: {df_processed.shape[1] - 1}\")  # -1 for target\n",
    "    print(f\"üéØ Total samples: {df_processed.shape[0]:,}\")\n",
    "    \n",
    "    return df_processed, le_dict\n",
    "\n",
    "# ============================================================================\n",
    "# 3. ENHANCED MODELING PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "class EnhancedFraudDetectionPipeline:\n",
    "    \"\"\"Enhanced fraud detection pipeline with better accuracy\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.scalers = {}\n",
    "        self.results = {}\n",
    "        self.best_model = None\n",
    "        self.best_score = 0\n",
    "        self.cv_results = {}\n",
    "        \n",
    "    def prepare_data_advanced(self, df_processed):\n",
    "        \"\"\"Advanced data preparation with multiple scaling options\"\"\"\n",
    "        \n",
    "        print(\"\\nüéØ ADVANCED DATA PREPARATION\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        # Separate features and target\n",
    "        X = df_processed.drop('Fraud_Label', axis=1)\n",
    "        y = df_processed['Fraud_Label']\n",
    "        \n",
    "        print(f\"Features shape: {X.shape}\")\n",
    "        print(f\"Target distribution: {y.value_counts().to_dict()}\")\n",
    "        print(f\"Fraud rate: {y.mean():.1%}\")\n",
    "        \n",
    "        # Advanced train-test split with stratification\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        # Multiple scaling approaches\n",
    "        # Standard scaling\n",
    "        standard_scaler = StandardScaler()\n",
    "        X_train_standard = standard_scaler.fit_transform(X_train)\n",
    "        X_test_standard = standard_scaler.transform(X_test)\n",
    "        \n",
    "        # Robust scaling (less sensitive to outliers)\n",
    "        robust_scaler = RobustScaler()\n",
    "        X_train_robust = robust_scaler.fit_transform(X_train)\n",
    "        X_test_robust = robust_scaler.transform(X_test)\n",
    "        \n",
    "        self.scalers['standard'] = standard_scaler\n",
    "        self.scalers['robust'] = robust_scaler\n",
    "        \n",
    "        return (X_train, X_test, y_train, y_test, \n",
    "                X_train_standard, X_test_standard,\n",
    "                X_train_robust, X_test_robust)\n",
    "    \n",
    "    def advanced_class_balancing(self, X_train_scaled, y_train):\n",
    "        \"\"\"Advanced class balancing techniques\"\"\"\n",
    "        \n",
    "        print(\"\\n‚öñÔ∏è ADVANCED CLASS BALANCING\")\n",
    "        print(\"=\" * 35)\n",
    "        \n",
    "        original_distribution = Counter(y_train)\n",
    "        print(f\"Original distribution: {dict(original_distribution)}\")\n",
    "        \n",
    "        balanced_datasets = {}\n",
    "        \n",
    "        if HAS_IMBLEARN:\n",
    "            # SMOTE variants\n",
    "            techniques = {\n",
    "                'SMOTE': SMOTE(random_state=42, k_neighbors=3),\n",
    "                'BorderlineSMOTE': BorderlineSMOTE(random_state=42, k_neighbors=3),\n",
    "                'ADASYN': ADASYN(random_state=42, n_neighbors=3),\n",
    "                'SMOTETomek': SMOTETomek(random_state=42)\n",
    "            }\n",
    "            \n",
    "            for name, technique in techniques.items():\n",
    "                try:\n",
    "                    X_balanced, y_balanced = technique.fit_resample(X_train_scaled, y_train)\n",
    "                    balanced_datasets[name] = (X_balanced, y_balanced)\n",
    "                    print(f\"‚úÖ {name}: {X_balanced.shape[0]:,} samples\")\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è {name} failed: {str(e)}\")\n",
    "        \n",
    "        # Fallback: Enhanced manual balancing\n",
    "        if not balanced_datasets:\n",
    "            print(\"Using enhanced manual balancing...\")\n",
    "            \n",
    "            # Separate classes\n",
    "            minority_class = y_train.value_counts().idxmin()\n",
    "            majority_class = y_train.value_counts().idxmax()\n",
    "            \n",
    "            minority_indices = np.where(y_train == minority_class)[0]\n",
    "            majority_indices = np.where(y_train == majority_class)[0]\n",
    "            \n",
    "            # Create balanced dataset with some noise\n",
    "            n_minority = len(minority_indices)\n",
    "            n_majority = len(majority_indices)\n",
    "            \n",
    "            # Oversample minority class with noise\n",
    "            minority_oversampled = []\n",
    "            for _ in range(n_majority - n_minority):\n",
    "                idx = np.random.choice(minority_indices)\n",
    "                sample = X_train_scaled[idx].copy()\n",
    "                # Add small amount of noise\n",
    "                noise = np.random.normal(0, 0.01, sample.shape)\n",
    "                minority_oversampled.append(sample + noise)\n",
    "            \n",
    "            if minority_oversampled:\n",
    "                X_minority_over = np.vstack([X_train_scaled[minority_indices]] + minority_oversampled)\n",
    "                y_minority_over = np.full(len(X_minority_over), minority_class)\n",
    "                \n",
    "                X_balanced = np.vstack([X_train_scaled[majority_indices], X_minority_over])\n",
    "                y_balanced = np.concatenate([y_train.iloc[majority_indices], y_minority_over])\n",
    "                \n",
    "                balanced_datasets['Manual'] = (X_balanced, y_balanced)\n",
    "                print(f\"‚úÖ Manual balancing: {X_balanced.shape[0]:,} samples\")\n",
    "        \n",
    "        # Return the best balanced dataset (prefer SMOTE variants)\n",
    "        if 'SMOTETomek' in balanced_datasets:\n",
    "            return balanced_datasets['SMOTETomek']\n",
    "        elif 'SMOTE' in balanced_datasets:\n",
    "            return balanced_datasets['SMOTE']\n",
    "        elif balanced_datasets:\n",
    "            return list(balanced_datasets.values())[0]\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No balancing applied, using original data\")\n",
    "            return X_train_scaled, y_train\n",
    "    \n",
    "    def build_enhanced_models(self, X_train_balanced, y_train_balanced, X_train_scaled):\n",
    "        \"\"\"Build enhanced models with hyperparameter tuning\"\"\"\n",
    "        \n",
    "        print(\"\\nü§ñ BUILDING ENHANCED MODELS\")\n",
    "        print(\"=\" * 35)\n",
    "        \n",
    "        # Enhanced model configurations\n",
    "        models_config = {\n",
    "            'Logistic Regression': {\n",
    "                'model': LogisticRegression(random_state=42, max_iter=2000),\n",
    "                'params': {\n",
    "                    'C': [0.1, 1, 10, 100],\n",
    "                    'penalty': ['l1', 'l2'],\n",
    "                    'solver': ['liblinear', 'saga']\n",
    "                }\n",
    "            },\n",
    "            'Random Forest': {\n",
    "                'model': RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "                'params': {\n",
    "                    'n_estimators': [100, 200, 300],\n",
    "                    'max_depth': [10, 20, None],\n",
    "                    'min_samples_split': [2, 5, 10],\n",
    "                    'min_samples_leaf': [1, 2, 4]\n",
    "                }\n",
    "            },\n",
    "            'Gradient Boosting': {\n",
    "                'model': GradientBoostingClassifier(random_state=42),\n",
    "                'params': {\n",
    "                    'n_estimators': [100, 200],\n",
    "                    'learning_rate': [0.05, 0.1, 0.2],\n",
    "                    'max_depth': [3, 5, 7]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Add XGBoost if available\n",
    "        if HAS_XGBOOST:\n",
    "            models_config['XGBoost'] = {\n",
    "                'model': xgb.XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "                'params': {\n",
    "                    'n_estimators': [100, 200],\n",
    "                    'learning_rate': [0.05, 0.1, 0.2],\n",
    "                    'max_depth': [3, 5, 7],\n",
    "                    'subsample': [0.8, 0.9, 1.0]\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        # Add CatBoost if available\n",
    "        if HAS_CATBOOST:\n",
    "            models_config['CatBoost'] = {\n",
    "                'model': CatBoostClassifier(random_state=42, verbose=False),\n",
    "                'params': {\n",
    "                    'iterations': [100, 200, 300],\n",
    "                    'learning_rate': [0.05, 0.1, 0.2],\n",
    "                    'depth': [4, 6, 8]\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        # Train models with hyperparameter tuning\n",
    "        for name, config in models_config.items():\n",
    "            print(f\"Training {name} with hyperparameter tuning...\")\n",
    "            try:\n",
    "                # Use GridSearchCV for hyperparameter tuning\n",
    "                grid_search = GridSearchCV(\n",
    "                    config['model'], \n",
    "                    config['params'],\n",
    "                    cv=3,  # 3-fold CV for speed\n",
    "                    scoring='f1',\n",
    "                    n_jobs=-1,\n",
    "                    verbose=0\n",
    "                )\n",
    "                \n",
    "                grid_search.fit(X_train_balanced, y_train_balanced)\n",
    "                \n",
    "                self.models[name] = grid_search.best_estimator_\n",
    "                self.cv_results[name] = {\n",
    "                    'best_score': grid_search.best_score_,\n",
    "                    'best_params': grid_search.best_params_\n",
    "                }\n",
    "                \n",
    "                print(f\"‚úÖ {name} - Best CV F1: {grid_search.best_score_:.4f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error training {name}: {e}\")\n",
    "                # Fallback to default parameters\n",
    "                try:\n",
    "                    config['model'].fit(X_train_balanced, y_train_balanced)\n",
    "                    self.models[name] = config['model']\n",
    "                    print(f\"‚úÖ {name} - Using default parameters\")\n",
    "                except Exception as e2:\n",
    "                    print(f\"‚ùå {name} failed completely: {e2}\")\n",
    "        \n",
    "        # Add ensemble methods\n",
    "        if len(self.models) >= 2:\n",
    "            print(\"Creating ensemble models...\")\n",
    "            # Voting classifier would go here, but skipping for simplicity\n",
    "        \n",
    "        return X_train_balanced, y_train_balanced\n",
    "    \n",
    "    def comprehensive_evaluation(self, X_test, y_test, X_test_scaled):\n",
    "        \"\"\"Comprehensive model evaluation with multiple metrics\"\"\"\n",
    "        \n",
    "        print(\"\\nüìä COMPREHENSIVE MODEL EVALUATION\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for name, model in self.models.items():\n",
    "            print(f\"\\nEvaluating {name}...\")\n",
    "            \n",
    "            try:\n",
    "                # Make predictions\n",
    "                y_pred = model.predict(X_test_scaled)\n",
    "                \n",
    "                if hasattr(model, 'predict_proba'):\n",
    "                    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "                else:\n",
    "                    # For models without predict_proba\n",
    "                    y_pred_proba = model.decision_function(X_test_scaled)\n",
    "                    # Normalize to [0,1]\n",
    "                    y_pred_proba = (y_pred_proba - y_pred_proba.min()) / (y_pred_proba.max() - y_pred_proba.min())\n",
    "                \n",
    "                # Calculate comprehensive metrics\n",
    "                precision = precision_score(y_test, y_pred)\n",
    "                recall = recall_score(y_test, y_pred)\n",
    "                f1 = f1_score(y_test, y_pred)\n",
    "                roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "                avg_precision = average_precision_score(y_test, y_pred_proba)\n",
    "                \n",
    "                # Additional metrics\n",
    "                tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "                specificity = tn / (tn + fp)\n",
    "                balanced_accuracy = (recall + specificity) / 2\n",
    "                \n",
    "                results.append({\n",
    "                    'Model': name,\n",
    "                    'Precision': precision,\n",
    "                    'Recall': recall,\n",
    "                    'F1-Score': f1,\n",
    "                    'ROC-AUC': roc_auc,\n",
    "                    'Avg-Precision': avg_precision,\n",
    "                    'Specificity': specificity,\n",
    "                    'Balanced-Acc': balanced_accuracy,\n",
    "                    'CV-F1': self.cv_results.get(name, {}).get('best_score', 0)\n",
    "                })\n",
    "                \n",
    "                # Store results\n",
    "                self.results[name] = {\n",
    "                    'y_pred': y_pred,\n",
    "                    'y_pred_proba': y_pred_proba,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'f1': f1,\n",
    "                    'roc_auc': roc_auc,\n",
    "                    'avg_precision': avg_precision,\n",
    "                    'specificity': specificity,\n",
    "                    'balanced_accuracy': balanced_accuracy\n",
    "                }\n",
    "                \n",
    "                # Update best model based on F1 score\n",
    "                if f1 > self.best_score:\n",
    "                    self.best_score = f1\n",
    "                    self.best_model = name\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error evaluating {name}: {e}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c68eff-e1e6-4683-949d-cbf6f6918570",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
